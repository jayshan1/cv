{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00b41ca399d84313b471507d8521e93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1c21a0aca70345cba72a3ef41e633f6c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_07661503c1424f978b00380a3eef4cde",
              "IPY_MODEL_2b68c435bba74d4997515cf23538b5ff"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "1c21a0aca70345cba72a3ef41e633f6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "07661503c1424f978b00380a3eef4cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1950a04ad9cd4c7483cd38d5538dc7af",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_860c1c98f5954a0b8d9ba45484170074"
          },
          "model_module_version": "1.5.0"
        },
        "2b68c435bba74d4997515cf23538b5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ee51f903597e46eeb13fac4ba50cad5a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 47863925.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27935aefcba84fb1ac804c62180c81d2"
          },
          "model_module_version": "1.5.0"
        },
        "1950a04ad9cd4c7483cd38d5538dc7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "860c1c98f5954a0b8d9ba45484170074": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "ee51f903597e46eeb13fac4ba50cad5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "27935aefcba84fb1ac804c62180c81d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgcUBh4YExcY"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tip_XdAYFGRK"
      },
      "source": [
        "\"\"\"\n",
        "    The following ResNet implementations are the official version of Pytorch library\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\n",
        "                'BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\n",
        "                \"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "def resnet_18_cifar():\n",
        "    r\"\"\"\n",
        "        Modify the resnet 18 network in order to run on cifar-10 dataset\n",
        "\n",
        "        To enhance the accuracy, the (kernel_size, stride, padding) of conv1 is modified to (3, 1, 1)\n",
        "        referenced by <https://github.com/akamaster/pytorch_resnet_cifar10>\n",
        "    \"\"\"\n",
        "    model = ResNet(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=10)\n",
        "\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
        "                            bias=False)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAkbl1GMclRd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7woqU9-ZF8CV",
        "outputId": "af898a21-ae1b-487f-f051-aa8f12706d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "00b41ca399d84313b471507d8521e93a",
            "1c21a0aca70345cba72a3ef41e633f6c",
            "07661503c1424f978b00380a3eef4cde",
            "2b68c435bba74d4997515cf23538b5ff",
            "1950a04ad9cd4c7483cd38d5538dc7af",
            "860c1c98f5954a0b8d9ba45484170074",
            "ee51f903597e46eeb13fac4ba50cad5a",
            "27935aefcba84fb1ac804c62180c81d2"
          ]
        }
      },
      "source": [
        "# load the data\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.RandomCrop(32, padding=4),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))])\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784))])\n",
        "\n",
        "ds = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "\n",
        "\n",
        "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "\n",
        "# split the training set and validation set\n",
        "torch.manual_seed(50)\n",
        "test_size = len(test_ds)\n",
        "val_size = 5000\n",
        "train_size = len(ds) - val_size\n",
        "batch_size = 256\n",
        "\n",
        "train_ds, val_ds = random_split(ds, [train_size, val_size])\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "classes = ds.classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00b41ca399d84313b471507d8521e93a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q7ghjGlUsIJ",
        "outputId": "a620e99c-a307-4633-ac26-4e4696bd6c8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_size)\n",
        "print(val_size)\n",
        "print(test_size)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45000\n",
            "5000\n",
            "10000\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxcYeH6Rj-uJ",
        "outputId": "6edbc04f-ec5f-4113-80cb-6d013f9fca51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp = torch.FloatTensor(next(iter(train_loader))[0])\n",
        "model = resnet_18_cifar()\n",
        "\n",
        "\n",
        "output = torch.onnx.export(model,tmp,'./data/resnet-18-cifar.onnx', verbose=False)\n",
        "print(\"Export of torch_model.onnx complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Export of torch_model.onnx complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5dB4kF7Gdva"
      },
      "source": [
        "import time\n",
        "\n",
        "def validate(model, criterion, val_loader, use_gpu=False):\n",
        "  val_size = len(val_loader.dataset)\n",
        "  val_loss = 0\n",
        "  correct = 0\n",
        "  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(val_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs).to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss * inputs.size(0)\n",
        "\n",
        "        # val accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss/val_size\n",
        "    val_accuracy = correct/val_size;\n",
        "\n",
        "  return val_loss, val_accuracy\n",
        "\n",
        "def get_train_accuracy(model, criterion, train_loader, use_gpu=False):\n",
        "   train_size = len(train_loader.dataset)\n",
        "   correct = 0\n",
        "   device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n",
        "\n",
        "   with torch.no_grad():\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs).to(device)\n",
        "\n",
        "        # val accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(correct)\n",
        "    return correct/train_size\n",
        "\n",
        "def train(model, criterion, optimizer, train_loader, val_loader, epoch=128, use_gpu=False):\n",
        "\n",
        "  train_size = len(train_loader.dataset)\n",
        "  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n",
        "\n",
        "  history = {\n",
        "      'train_loss': [],\n",
        "      'train_accuracy': [],\n",
        "      'val_loss': [],\n",
        "      'val_accuracy': []\n",
        "      }\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  for epoch in range(epoch):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    print(f'------------------------------\\n Epoch: {epoch + 1}')\n",
        "\n",
        "    t1 = time.time()\n",
        "    for  i,data in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # as loss.item() return the average batch loss, so convert it to the total loss\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    t2 = time.time()\n",
        "    t = t2 - t1\n",
        "\n",
        "    # save the model\n",
        "    PATH = f'./drive/My Drive/Colab Notebooks/checkpoints/resnet18-original-e{epoch}.pth'\n",
        "    if epoch % 10 == 0:\n",
        "      torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),}, PATH)\n",
        "\n",
        "    epoch_train_loss = running_loss/train_size\n",
        "    epoch_train_accuracy = get_train_accuracy(model, criterion, train_loader, use_gpu)\n",
        "    epoch_val_loss, epoch_val_accuracy = validate(model, criterion, val_loader, use_gpu)\n",
        "    print(f'time: {int(t)}sec train_loss: {epoch_train_loss}, train_accuracy: {epoch_train_accuracy}, val_loss: {epoch_val_loss}, val_accuracy: {epoch_val_accuracy}');\n",
        "\n",
        "    history['train_loss'].append(epoch_train_loss)\n",
        "    history['train_accuracy'].append(epoch_train_accuracy)\n",
        "    history['val_loss'].append(epoch_val_loss)\n",
        "    history['val_accuracy'].append(epoch_val_accuracy)\n",
        "\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCBeSMOeGiAT",
        "outputId": "65c7edcb-d8ac-4ac8-e733-39b17e02a1cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define a model\n",
        "model = resnet_18_cifar()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "history = train(model, criterion, optimizer, train_loader, val_loader, 128, torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            " Epoch: 1\n",
            "22996\n",
            "time: 25sec train_loss: 1.613640360959371, train_accuracy: 0.5110222222222223, val_loss: 1.362740397453308, val_accuracy: 0.4946\n",
            "------------------------------\n",
            " Epoch: 2\n",
            "26859\n",
            "time: 25sec train_loss: 1.2283708485709297, train_accuracy: 0.5968666666666667, val_loss: 1.1388534307479858, val_accuracy: 0.59\n",
            "------------------------------\n",
            " Epoch: 3\n",
            "29096\n",
            "time: 25sec train_loss: 1.0499053016874524, train_accuracy: 0.6465777777777778, val_loss: 1.0283063650131226, val_accuracy: 0.6346\n",
            "------------------------------\n",
            " Epoch: 4\n",
            "31548\n",
            "time: 25sec train_loss: 0.9144144903182984, train_accuracy: 0.7010666666666666, val_loss: 0.8811872601509094, val_accuracy: 0.6836\n",
            "------------------------------\n",
            " Epoch: 5\n",
            "32108\n",
            "time: 25sec train_loss: 0.8269177289644877, train_accuracy: 0.7135111111111111, val_loss: 0.8425401449203491, val_accuracy: 0.703\n",
            "------------------------------\n",
            " Epoch: 6\n",
            "32839\n",
            "time: 25sec train_loss: 0.7582611115667555, train_accuracy: 0.7297555555555556, val_loss: 0.8204214572906494, val_accuracy: 0.7088\n",
            "------------------------------\n",
            " Epoch: 7\n",
            "34483\n",
            "time: 25sec train_loss: 0.705005151632097, train_accuracy: 0.7662888888888889, val_loss: 0.7234662771224976, val_accuracy: 0.7452\n",
            "------------------------------\n",
            " Epoch: 8\n",
            "35209\n",
            "time: 25sec train_loss: 0.6613386711014642, train_accuracy: 0.7824222222222222, val_loss: 0.6847584843635559, val_accuracy: 0.7624\n",
            "------------------------------\n",
            " Epoch: 9\n",
            "36026\n",
            "time: 25sec train_loss: 0.6220583531061809, train_accuracy: 0.8005777777777778, val_loss: 0.6554326415061951, val_accuracy: 0.7726\n",
            "------------------------------\n",
            " Epoch: 10\n",
            "35416\n",
            "time: 25sec train_loss: 0.5828883697191874, train_accuracy: 0.7870222222222222, val_loss: 0.6859325170516968, val_accuracy: 0.759\n",
            "------------------------------\n",
            " Epoch: 11\n",
            "36788\n",
            "time: 25sec train_loss: 0.562651193745931, train_accuracy: 0.8175111111111111, val_loss: 0.6138721704483032, val_accuracy: 0.7908\n",
            "------------------------------\n",
            " Epoch: 12\n",
            "37177\n",
            "time: 25sec train_loss: 0.5308218165821499, train_accuracy: 0.8261555555555555, val_loss: 0.5880663394927979, val_accuracy: 0.7982\n",
            "------------------------------\n",
            " Epoch: 13\n",
            "37521\n",
            "time: 25sec train_loss: 0.5082344223764208, train_accuracy: 0.8338, val_loss: 0.5725798606872559, val_accuracy: 0.8008\n",
            "------------------------------\n",
            " Epoch: 14\n",
            "37617\n",
            "time: 25sec train_loss: 0.4883022555033366, train_accuracy: 0.8359333333333333, val_loss: 0.5842728018760681, val_accuracy: 0.7992\n",
            "------------------------------\n",
            " Epoch: 15\n",
            "38185\n",
            "time: 25sec train_loss: 0.47458481494055854, train_accuracy: 0.8485555555555555, val_loss: 0.5767714381217957, val_accuracy: 0.8044\n",
            "------------------------------\n",
            " Epoch: 16\n",
            "38110\n",
            "time: 25sec train_loss: 0.44902403138478597, train_accuracy: 0.8468888888888889, val_loss: 0.5797215700149536, val_accuracy: 0.7974\n",
            "------------------------------\n",
            " Epoch: 17\n",
            "38658\n",
            "time: 25sec train_loss: 0.4335690370188819, train_accuracy: 0.8590666666666666, val_loss: 0.5468575954437256, val_accuracy: 0.8188\n",
            "------------------------------\n",
            " Epoch: 18\n",
            "38915\n",
            "time: 25sec train_loss: 0.4138186957783169, train_accuracy: 0.8647777777777778, val_loss: 0.5331828594207764, val_accuracy: 0.8218\n",
            "------------------------------\n",
            " Epoch: 19\n",
            "38887\n",
            "time: 25sec train_loss: 0.39884634767108496, train_accuracy: 0.8641555555555556, val_loss: 0.5394926071166992, val_accuracy: 0.8156\n",
            "------------------------------\n",
            " Epoch: 20\n",
            "39515\n",
            "time: 25sec train_loss: 0.3842529888365004, train_accuracy: 0.8781111111111111, val_loss: 0.5004135370254517, val_accuracy: 0.832\n",
            "------------------------------\n",
            " Epoch: 21\n",
            "39642\n",
            "time: 25sec train_loss: 0.3694262097729577, train_accuracy: 0.8809333333333333, val_loss: 0.5144041776657104, val_accuracy: 0.8238\n",
            "------------------------------\n",
            " Epoch: 22\n",
            "39646\n",
            "time: 25sec train_loss: 0.3564034129778544, train_accuracy: 0.8810222222222223, val_loss: 0.5206304788589478, val_accuracy: 0.8318\n",
            "------------------------------\n",
            " Epoch: 23\n",
            "40019\n",
            "time: 25sec train_loss: 0.3434636012713114, train_accuracy: 0.8893111111111112, val_loss: 0.5047823786735535, val_accuracy: 0.8274\n",
            "------------------------------\n",
            " Epoch: 24\n",
            "40047\n",
            "time: 25sec train_loss: 0.32943038069407143, train_accuracy: 0.8899333333333334, val_loss: 0.5005008578300476, val_accuracy: 0.8294\n",
            "------------------------------\n",
            " Epoch: 25\n",
            "40216\n",
            "time: 25sec train_loss: 0.3247116689576043, train_accuracy: 0.8936888888888889, val_loss: 0.4869975447654724, val_accuracy: 0.8312\n",
            "------------------------------\n",
            " Epoch: 26\n",
            "40454\n",
            "time: 25sec train_loss: 0.3127702699078454, train_accuracy: 0.8989777777777778, val_loss: 0.46982163190841675, val_accuracy: 0.8424\n",
            "------------------------------\n",
            " Epoch: 27\n",
            "40777\n",
            "time: 25sec train_loss: 0.3023429467148251, train_accuracy: 0.9061555555555556, val_loss: 0.47109904885292053, val_accuracy: 0.8408\n",
            "------------------------------\n",
            " Epoch: 28\n",
            "40723\n",
            "time: 25sec train_loss: 0.2963168700059255, train_accuracy: 0.9049555555555555, val_loss: 0.4861157536506653, val_accuracy: 0.8424\n",
            "------------------------------\n",
            " Epoch: 29\n",
            "40685\n",
            "time: 25sec train_loss: 0.2806270526885986, train_accuracy: 0.9041111111111111, val_loss: 0.49086806178092957, val_accuracy: 0.8398\n",
            "------------------------------\n",
            " Epoch: 30\n",
            "41074\n",
            "time: 25sec train_loss: 0.2671865075800154, train_accuracy: 0.9127555555555555, val_loss: 0.46744412183761597, val_accuracy: 0.8432\n",
            "------------------------------\n",
            " Epoch: 31\n",
            "41117\n",
            "time: 25sec train_loss: 0.2702420668654972, train_accuracy: 0.9137111111111111, val_loss: 0.46465638279914856, val_accuracy: 0.8446\n",
            "------------------------------\n",
            " Epoch: 32\n",
            "41044\n",
            "time: 25sec train_loss: 0.2585345060242547, train_accuracy: 0.9120888888888888, val_loss: 0.4842190742492676, val_accuracy: 0.8482\n",
            "------------------------------\n",
            " Epoch: 33\n",
            "41248\n",
            "time: 25sec train_loss: 0.2462608693705665, train_accuracy: 0.9166222222222222, val_loss: 0.4692065417766571, val_accuracy: 0.8474\n",
            "------------------------------\n",
            " Epoch: 34\n",
            "41362\n",
            "time: 25sec train_loss: 0.24162153367996217, train_accuracy: 0.9191555555555555, val_loss: 0.4650062024593353, val_accuracy: 0.8442\n",
            "------------------------------\n",
            " Epoch: 35\n",
            "41512\n",
            "time: 25sec train_loss: 0.23992850567499796, train_accuracy: 0.9224888888888889, val_loss: 0.4668169319629669, val_accuracy: 0.851\n",
            "------------------------------\n",
            " Epoch: 36\n",
            "41600\n",
            "time: 25sec train_loss: 0.22269911776648627, train_accuracy: 0.9244444444444444, val_loss: 0.46874499320983887, val_accuracy: 0.8544\n",
            "------------------------------\n",
            " Epoch: 37\n",
            "41946\n",
            "time: 25sec train_loss: 0.22713293143908184, train_accuracy: 0.9321333333333334, val_loss: 0.45378080010414124, val_accuracy: 0.8512\n",
            "------------------------------\n",
            " Epoch: 38\n",
            "41976\n",
            "time: 25sec train_loss: 0.2192322484811147, train_accuracy: 0.9328, val_loss: 0.43404409289360046, val_accuracy: 0.8588\n",
            "------------------------------\n",
            " Epoch: 39\n",
            "42036\n",
            "time: 25sec train_loss: 0.20240560262997945, train_accuracy: 0.9341333333333334, val_loss: 0.44420385360717773, val_accuracy: 0.862\n",
            "------------------------------\n",
            " Epoch: 40\n",
            "42058\n",
            "time: 25sec train_loss: 0.19968169439633687, train_accuracy: 0.9346222222222222, val_loss: 0.4681970477104187, val_accuracy: 0.8548\n",
            "------------------------------\n",
            " Epoch: 41\n",
            "42338\n",
            "time: 25sec train_loss: 0.19311974494722153, train_accuracy: 0.9408444444444445, val_loss: 0.4565165936946869, val_accuracy: 0.8526\n",
            "------------------------------\n",
            " Epoch: 42\n",
            "42073\n",
            "time: 25sec train_loss: 0.19078194691340128, train_accuracy: 0.9349555555555555, val_loss: 0.45992177724838257, val_accuracy: 0.8554\n",
            "------------------------------\n",
            " Epoch: 43\n",
            "42314\n",
            "time: 25sec train_loss: 0.1935241606288486, train_accuracy: 0.9403111111111111, val_loss: 0.45505091547966003, val_accuracy: 0.8558\n",
            "------------------------------\n",
            " Epoch: 44\n",
            "42385\n",
            "time: 25sec train_loss: 0.18412758010228475, train_accuracy: 0.9418888888888889, val_loss: 0.4762805998325348, val_accuracy: 0.8548\n",
            "------------------------------\n",
            " Epoch: 45\n",
            "42710\n",
            "time: 25sec train_loss: 0.17444002678261863, train_accuracy: 0.9491111111111111, val_loss: 0.4480312764644623, val_accuracy: 0.861\n",
            "------------------------------\n",
            " Epoch: 46\n",
            "42611\n",
            "time: 25sec train_loss: 0.17203170714908175, train_accuracy: 0.9469111111111111, val_loss: 0.44311296939849854, val_accuracy: 0.8658\n",
            "------------------------------\n",
            " Epoch: 47\n",
            "42400\n",
            "time: 25sec train_loss: 0.167222929403517, train_accuracy: 0.9422222222222222, val_loss: 0.47117263078689575, val_accuracy: 0.8572\n",
            "------------------------------\n",
            " Epoch: 48\n",
            "42562\n",
            "time: 25sec train_loss: 0.16216427059703403, train_accuracy: 0.9458222222222222, val_loss: 0.4761451482772827, val_accuracy: 0.8552\n",
            "------------------------------\n",
            " Epoch: 49\n",
            "42818\n",
            "time: 25sec train_loss: 0.16067095220618777, train_accuracy: 0.9515111111111111, val_loss: 0.4540432393550873, val_accuracy: 0.8662\n",
            "------------------------------\n",
            " Epoch: 50\n",
            "42569\n",
            "time: 25sec train_loss: 0.1563289545589023, train_accuracy: 0.9459777777777778, val_loss: 0.4683679938316345, val_accuracy: 0.861\n",
            "------------------------------\n",
            " Epoch: 51\n",
            "42702\n",
            "time: 25sec train_loss: 0.1486765150414573, train_accuracy: 0.9489333333333333, val_loss: 0.46789565682411194, val_accuracy: 0.861\n",
            "------------------------------\n",
            " Epoch: 52\n",
            "42723\n",
            "time: 25sec train_loss: 0.15023658225801256, train_accuracy: 0.9494, val_loss: 0.4706168472766876, val_accuracy: 0.8592\n",
            "------------------------------\n",
            " Epoch: 53\n",
            "42851\n",
            "time: 25sec train_loss: 0.14568878586822087, train_accuracy: 0.9522444444444444, val_loss: 0.4443313777446747, val_accuracy: 0.8644\n",
            "------------------------------\n",
            " Epoch: 54\n",
            "42858\n",
            "time: 25sec train_loss: 0.13984498554865518, train_accuracy: 0.9524, val_loss: 0.4638640880584717, val_accuracy: 0.8598\n",
            "------------------------------\n",
            " Epoch: 55\n",
            "43216\n",
            "time: 25sec train_loss: 0.14062589804066553, train_accuracy: 0.9603555555555555, val_loss: 0.4518533945083618, val_accuracy: 0.8672\n",
            "------------------------------\n",
            " Epoch: 56\n",
            "43115\n",
            "time: 25sec train_loss: 0.1327827419810825, train_accuracy: 0.9581111111111111, val_loss: 0.4557639956474304, val_accuracy: 0.8654\n",
            "------------------------------\n",
            " Epoch: 57\n",
            "42903\n",
            "time: 25sec train_loss: 0.1355678449312846, train_accuracy: 0.9534, val_loss: 0.4362054169178009, val_accuracy: 0.8694\n",
            "------------------------------\n",
            " Epoch: 58\n",
            "43159\n",
            "time: 25sec train_loss: 0.12557115690112114, train_accuracy: 0.9590888888888889, val_loss: 0.4588865041732788, val_accuracy: 0.8654\n",
            "------------------------------\n",
            " Epoch: 59\n",
            "43243\n",
            "time: 25sec train_loss: 0.12410357944170634, train_accuracy: 0.9609555555555556, val_loss: 0.46465685963630676, val_accuracy: 0.8686\n",
            "------------------------------\n",
            " Epoch: 60\n",
            "43075\n",
            "time: 25sec train_loss: 0.12332957370811039, train_accuracy: 0.9572222222222222, val_loss: 0.48003166913986206, val_accuracy: 0.8604\n",
            "------------------------------\n",
            " Epoch: 61\n",
            "43047\n",
            "time: 25sec train_loss: 0.1226818991223971, train_accuracy: 0.9566, val_loss: 0.485710084438324, val_accuracy: 0.863\n",
            "------------------------------\n",
            " Epoch: 62\n",
            "43292\n",
            "time: 25sec train_loss: 0.1245788459473186, train_accuracy: 0.9620444444444445, val_loss: 0.4591657221317291, val_accuracy: 0.8636\n",
            "------------------------------\n",
            " Epoch: 63\n",
            "43308\n",
            "time: 25sec train_loss: 0.11593469936847686, train_accuracy: 0.9624, val_loss: 0.4600350558757782, val_accuracy: 0.867\n",
            "------------------------------\n",
            " Epoch: 64\n",
            "43252\n",
            "time: 25sec train_loss: 0.11737759997977151, train_accuracy: 0.9611555555555555, val_loss: 0.45119476318359375, val_accuracy: 0.8676\n",
            "------------------------------\n",
            " Epoch: 65\n",
            "43427\n",
            "time: 25sec train_loss: 0.11551463497744666, train_accuracy: 0.9650444444444445, val_loss: 0.45929089188575745, val_accuracy: 0.867\n",
            "------------------------------\n",
            " Epoch: 66\n",
            "43362\n",
            "time: 25sec train_loss: 0.11001823253631592, train_accuracy: 0.9636, val_loss: 0.4683956503868103, val_accuracy: 0.8646\n",
            "------------------------------\n",
            " Epoch: 67\n",
            "43356\n",
            "time: 25sec train_loss: 0.10239488161404928, train_accuracy: 0.9634666666666667, val_loss: 0.4848462641239166, val_accuracy: 0.863\n",
            "------------------------------\n",
            " Epoch: 68\n",
            "43476\n",
            "time: 25sec train_loss: 0.10871405925353368, train_accuracy: 0.9661333333333333, val_loss: 0.4693242013454437, val_accuracy: 0.8684\n",
            "------------------------------\n",
            " Epoch: 69\n",
            "43422\n",
            "time: 25sec train_loss: 0.10498022831413481, train_accuracy: 0.9649333333333333, val_loss: 0.4660027325153351, val_accuracy: 0.8704\n",
            "------------------------------\n",
            " Epoch: 70\n",
            "43503\n",
            "time: 25sec train_loss: 0.10209825056261486, train_accuracy: 0.9667333333333333, val_loss: 0.4549395442008972, val_accuracy: 0.8728\n",
            "------------------------------\n",
            " Epoch: 71\n",
            "43589\n",
            "time: 25sec train_loss: 0.10000401848024792, train_accuracy: 0.9686444444444444, val_loss: 0.4770105481147766, val_accuracy: 0.8708\n",
            "------------------------------\n",
            " Epoch: 72\n",
            "43527\n",
            "time: 25sec train_loss: 0.10181732183297475, train_accuracy: 0.9672666666666667, val_loss: 0.46828722953796387, val_accuracy: 0.8694\n",
            "------------------------------\n",
            " Epoch: 73\n",
            "43610\n",
            "time: 25sec train_loss: 0.10028133815394508, train_accuracy: 0.9691111111111111, val_loss: 0.4366234242916107, val_accuracy: 0.876\n",
            "------------------------------\n",
            " Epoch: 74\n",
            "43646\n",
            "time: 25sec train_loss: 0.0979418728960885, train_accuracy: 0.9699111111111111, val_loss: 0.44968894124031067, val_accuracy: 0.8746\n",
            "------------------------------\n",
            " Epoch: 75\n",
            "43371\n",
            "time: 25sec train_loss: 0.0938259978334109, train_accuracy: 0.9638, val_loss: 0.4813378155231476, val_accuracy: 0.8704\n",
            "------------------------------\n",
            " Epoch: 76\n",
            "43566\n",
            "time: 25sec train_loss: 0.09723953979942533, train_accuracy: 0.9681333333333333, val_loss: 0.47251445055007935, val_accuracy: 0.8678\n",
            "------------------------------\n",
            " Epoch: 77\n",
            "43832\n",
            "time: 25sec train_loss: 0.0880188344584571, train_accuracy: 0.9740444444444445, val_loss: 0.4596657156944275, val_accuracy: 0.8724\n",
            "------------------------------\n",
            " Epoch: 78\n",
            "43827\n",
            "time: 25sec train_loss: 0.08786217184066772, train_accuracy: 0.9739333333333333, val_loss: 0.4517837166786194, val_accuracy: 0.874\n",
            "------------------------------\n",
            " Epoch: 79\n",
            "43604\n",
            "time: 25sec train_loss: 0.08626257210440105, train_accuracy: 0.9689777777777778, val_loss: 0.49239739775657654, val_accuracy: 0.8686\n",
            "------------------------------\n",
            " Epoch: 80\n",
            "43759\n",
            "time: 25sec train_loss: 0.08894675810601976, train_accuracy: 0.9724222222222222, val_loss: 0.4949965178966522, val_accuracy: 0.8708\n",
            "------------------------------\n",
            " Epoch: 81\n",
            "43709\n",
            "time: 25sec train_loss: 0.08962038865486781, train_accuracy: 0.9713111111111111, val_loss: 0.48042425513267517, val_accuracy: 0.8706\n",
            "------------------------------\n",
            " Epoch: 82\n",
            "43892\n",
            "time: 25sec train_loss: 0.09110605154302386, train_accuracy: 0.9753777777777778, val_loss: 0.45764362812042236, val_accuracy: 0.8742\n",
            "------------------------------\n",
            " Epoch: 83\n",
            "43716\n",
            "time: 25sec train_loss: 0.07888083299530876, train_accuracy: 0.9714666666666667, val_loss: 0.46918055415153503, val_accuracy: 0.869\n",
            "------------------------------\n",
            " Epoch: 84\n",
            "43708\n",
            "time: 25sec train_loss: 0.07979328395260706, train_accuracy: 0.9712888888888889, val_loss: 0.5130496025085449, val_accuracy: 0.8654\n",
            "------------------------------\n",
            " Epoch: 85\n",
            "43704\n",
            "time: 25sec train_loss: 0.08438061786492665, train_accuracy: 0.9712, val_loss: 0.5037152171134949, val_accuracy: 0.8628\n",
            "------------------------------\n",
            " Epoch: 86\n",
            "43874\n",
            "time: 25sec train_loss: 0.0842716184867753, train_accuracy: 0.9749777777777778, val_loss: 0.4463749825954437, val_accuracy: 0.8756\n",
            "------------------------------\n",
            " Epoch: 87\n",
            "43929\n",
            "time: 25sec train_loss: 0.07966135182380676, train_accuracy: 0.9762, val_loss: 0.44757533073425293, val_accuracy: 0.8768\n",
            "------------------------------\n",
            " Epoch: 88\n",
            "44039\n",
            "time: 25sec train_loss: 0.08176558867428038, train_accuracy: 0.9786444444444444, val_loss: 0.42729511857032776, val_accuracy: 0.8824\n",
            "------------------------------\n",
            " Epoch: 89\n",
            "43800\n",
            "time: 25sec train_loss: 0.07961658527188831, train_accuracy: 0.9733333333333334, val_loss: 0.46900007128715515, val_accuracy: 0.8696\n",
            "------------------------------\n",
            " Epoch: 90\n",
            "43698\n",
            "time: 25sec train_loss: 0.07398099483648936, train_accuracy: 0.9710666666666666, val_loss: 0.49372097849845886, val_accuracy: 0.8718\n",
            "------------------------------\n",
            " Epoch: 91\n",
            "43430\n",
            "time: 25sec train_loss: 0.0847941716644499, train_accuracy: 0.9651111111111111, val_loss: 0.4931783676147461, val_accuracy: 0.8648\n",
            "------------------------------\n",
            " Epoch: 92\n",
            "43984\n",
            "time: 25sec train_loss: 0.0831587278313107, train_accuracy: 0.9774222222222222, val_loss: 0.43724125623703003, val_accuracy: 0.8828\n",
            "------------------------------\n",
            " Epoch: 93\n",
            "43954\n",
            "time: 25sec train_loss: 0.07181306539509032, train_accuracy: 0.9767555555555556, val_loss: 0.4616500437259674, val_accuracy: 0.8766\n",
            "------------------------------\n",
            " Epoch: 94\n",
            "44030\n",
            "time: 25sec train_loss: 0.06759043750630485, train_accuracy: 0.9784444444444444, val_loss: 0.46201077103614807, val_accuracy: 0.8806\n",
            "------------------------------\n",
            " Epoch: 95\n",
            "44032\n",
            "time: 25sec train_loss: 0.0726446806497044, train_accuracy: 0.9784888888888889, val_loss: 0.4645954966545105, val_accuracy: 0.8794\n",
            "------------------------------\n",
            " Epoch: 96\n",
            "43901\n",
            "time: 25sec train_loss: 0.0774914290646712, train_accuracy: 0.9755777777777778, val_loss: 0.4781927168369293, val_accuracy: 0.8742\n",
            "------------------------------\n",
            " Epoch: 97\n",
            "43890\n",
            "time: 25sec train_loss: 0.076355060394605, train_accuracy: 0.9753333333333334, val_loss: 0.45488592982292175, val_accuracy: 0.8804\n",
            "------------------------------\n",
            " Epoch: 98\n",
            "43899\n",
            "time: 25sec train_loss: 0.07208093752463658, train_accuracy: 0.9755333333333334, val_loss: 0.4468809962272644, val_accuracy: 0.8852\n",
            "------------------------------\n",
            " Epoch: 99\n",
            "44112\n",
            "time: 25sec train_loss: 0.07212627278102769, train_accuracy: 0.9802666666666666, val_loss: 0.43831878900527954, val_accuracy: 0.8816\n",
            "------------------------------\n",
            " Epoch: 100\n",
            "44044\n",
            "time: 25sec train_loss: 0.07030548604329427, train_accuracy: 0.9787555555555556, val_loss: 0.4835132658481598, val_accuracy: 0.8766\n",
            "------------------------------\n",
            " Epoch: 101\n",
            "43884\n",
            "time: 25sec train_loss: 0.06837527547147539, train_accuracy: 0.9752, val_loss: 0.47111380100250244, val_accuracy: 0.87\n",
            "------------------------------\n",
            " Epoch: 102\n",
            "44120\n",
            "time: 25sec train_loss: 0.06731036826769511, train_accuracy: 0.9804444444444445, val_loss: 0.4557507336139679, val_accuracy: 0.8824\n",
            "------------------------------\n",
            " Epoch: 103\n",
            "43943\n",
            "time: 25sec train_loss: 0.0683320204867257, train_accuracy: 0.9765111111111111, val_loss: 0.48132234811782837, val_accuracy: 0.874\n",
            "------------------------------\n",
            " Epoch: 104\n",
            "44182\n",
            "time: 25sec train_loss: 0.06570608282619052, train_accuracy: 0.9818222222222223, val_loss: 0.4425087869167328, val_accuracy: 0.879\n",
            "------------------------------\n",
            " Epoch: 105\n",
            "43979\n",
            "time: 25sec train_loss: 0.06242795358167754, train_accuracy: 0.9773111111111111, val_loss: 0.476930171251297, val_accuracy: 0.874\n",
            "------------------------------\n",
            " Epoch: 106\n",
            "43953\n",
            "time: 25sec train_loss: 0.06814489960670471, train_accuracy: 0.9767333333333333, val_loss: 0.4948278069496155, val_accuracy: 0.8734\n",
            "------------------------------\n",
            " Epoch: 107\n",
            "44113\n",
            "time: 25sec train_loss: 0.0735532766620318, train_accuracy: 0.9802888888888889, val_loss: 0.46819818019866943, val_accuracy: 0.8774\n",
            "------------------------------\n",
            " Epoch: 108\n",
            "43984\n",
            "time: 25sec train_loss: 0.06634578589995702, train_accuracy: 0.9774222222222222, val_loss: 0.44497591257095337, val_accuracy: 0.8804\n",
            "------------------------------\n",
            " Epoch: 109\n",
            "44082\n",
            "time: 25sec train_loss: 0.0640301034755177, train_accuracy: 0.9796, val_loss: 0.44414612650871277, val_accuracy: 0.8788\n",
            "------------------------------\n",
            " Epoch: 110\n",
            "44176\n",
            "time: 25sec train_loss: 0.06758581878211763, train_accuracy: 0.9816888888888889, val_loss: 0.4847323000431061, val_accuracy: 0.8774\n",
            "------------------------------\n",
            " Epoch: 111\n",
            "44029\n",
            "time: 25sec train_loss: 0.06332157975170348, train_accuracy: 0.9784222222222222, val_loss: 0.46263596415519714, val_accuracy: 0.881\n",
            "------------------------------\n",
            " Epoch: 112\n",
            "44018\n",
            "time: 25sec train_loss: 0.06377493268118964, train_accuracy: 0.9781777777777778, val_loss: 0.4526316821575165, val_accuracy: 0.878\n",
            "------------------------------\n",
            " Epoch: 113\n",
            "43972\n",
            "time: 25sec train_loss: 0.06335328254964616, train_accuracy: 0.9771555555555556, val_loss: 0.47194164991378784, val_accuracy: 0.8742\n",
            "------------------------------\n",
            " Epoch: 114\n",
            "44005\n",
            "time: 25sec train_loss: 0.06952347294886907, train_accuracy: 0.9778888888888889, val_loss: 0.469820111989975, val_accuracy: 0.8734\n",
            "------------------------------\n",
            " Epoch: 115\n",
            "44206\n",
            "time: 25sec train_loss: 0.065473061611255, train_accuracy: 0.9823555555555555, val_loss: 0.46885672211647034, val_accuracy: 0.8772\n",
            "------------------------------\n",
            " Epoch: 116\n",
            "44139\n",
            "time: 25sec train_loss: 0.06124391406575839, train_accuracy: 0.9808666666666667, val_loss: 0.45646288990974426, val_accuracy: 0.8784\n",
            "------------------------------\n",
            " Epoch: 117\n",
            "44104\n",
            "time: 25sec train_loss: 0.06738830910523733, train_accuracy: 0.9800888888888889, val_loss: 0.45026081800460815, val_accuracy: 0.877\n",
            "------------------------------\n",
            " Epoch: 118\n",
            "44167\n",
            "time: 25sec train_loss: 0.06172022671831979, train_accuracy: 0.9814888888888889, val_loss: 0.475680947303772, val_accuracy: 0.875\n",
            "------------------------------\n",
            " Epoch: 119\n",
            "44210\n",
            "time: 25sec train_loss: 0.05939981921315193, train_accuracy: 0.9824444444444445, val_loss: 0.4622551202774048, val_accuracy: 0.88\n",
            "------------------------------\n",
            " Epoch: 120\n",
            "44048\n",
            "time: 25sec train_loss: 0.06475410325460963, train_accuracy: 0.9788444444444444, val_loss: 0.46738719940185547, val_accuracy: 0.8744\n",
            "------------------------------\n",
            " Epoch: 121\n",
            "44088\n",
            "time: 25sec train_loss: 0.06213550536698765, train_accuracy: 0.9797333333333333, val_loss: 0.46405214071273804, val_accuracy: 0.8772\n",
            "------------------------------\n",
            " Epoch: 122\n",
            "44154\n",
            "time: 25sec train_loss: 0.06823551994959513, train_accuracy: 0.9812, val_loss: 0.47034406661987305, val_accuracy: 0.871\n",
            "------------------------------\n",
            " Epoch: 123\n",
            "44268\n",
            "time: 25sec train_loss: 0.05951501940753725, train_accuracy: 0.9837333333333333, val_loss: 0.46219950914382935, val_accuracy: 0.882\n",
            "------------------------------\n",
            " Epoch: 124\n",
            "44122\n",
            "time: 25sec train_loss: 0.0629230667538113, train_accuracy: 0.9804888888888889, val_loss: 0.45955008268356323, val_accuracy: 0.8784\n",
            "------------------------------\n",
            " Epoch: 125\n",
            "44278\n",
            "time: 25sec train_loss: 0.05545167945623398, train_accuracy: 0.9839555555555556, val_loss: 0.4465508759021759, val_accuracy: 0.8834\n",
            "------------------------------\n",
            " Epoch: 126\n",
            "43971\n",
            "time: 25sec train_loss: 0.053875109244717494, train_accuracy: 0.9771333333333333, val_loss: 0.4805741012096405, val_accuracy: 0.879\n",
            "------------------------------\n",
            " Epoch: 127\n",
            "44271\n",
            "time: 25sec train_loss: 0.06169357383052508, train_accuracy: 0.9838, val_loss: 0.4393490254878998, val_accuracy: 0.8822\n",
            "------------------------------\n",
            " Epoch: 128\n",
            "44122\n",
            "time: 25sec train_loss: 0.06202445824278725, train_accuracy: 0.9804888888888889, val_loss: 0.46116727590560913, val_accuracy: 0.8768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8zLRzNjQkrn"
      },
      "source": [
        "PATH = f'./drive/My Drive/Colab Notebooks/checkpoints/resnet18-original-e128.pth'\n",
        "torch.save({'epoch': 128, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),}, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RzDqTWF6JAk"
      },
      "source": [
        "import pickle as pk\n",
        "\n",
        "with open('./baseline_train_params.pkl', 'wb') as f:\n",
        "  pk.dump({\n",
        "      'history': history,\n",
        "      'num_of_epoch': 128,\n",
        "      'lr': 0.001,\n",
        "      'momentum': 0.9,\n",
        "      'weight_decay':5e-4\n",
        "  }, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8H5saCYrzIM"
      },
      "source": [
        "with open('./baseline_train_params.pkl', 'rb') as f:\n",
        "  train_params = pk.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "240tq2HX-N6M"
      },
      "source": [
        "history = train_params['history']\n",
        "history['val_loss'] = [item.item() for item in train_params['history']['val_loss']]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9abEvxn-r6X"
      },
      "source": [
        "# run test set\n",
        "def test(model, criterion, test_loader, use_gpu=False):\n",
        "  test_size = len(test_loader.dataset)\n",
        "  device = torch.device( \"cuda:0\" if use_gpu else \"cpu\" )\n",
        "  test_loss = 0.0\n",
        "  test_accuracy = 0\n",
        "  correct = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs).to(device)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          test_loss += loss * inputs.size(0)\n",
        "\n",
        "          # val accuracy\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "      test_loss = test_loss/test_size\n",
        "      test_accuracy = correct/test_size;\n",
        "\n",
        "  return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2euJi6fBJdb"
      },
      "source": [
        "test_loss, test_accuracy = test(model, criterion, test_loader, True)\n",
        "history['test_loss'] = test_loss\n",
        "history['test_accuracy'] = test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NNBClh2BhtN",
        "outputId": "9c85c43a-63c3-4235-a380-71b7fca350a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(history['test_loss'], history['test_accuracy'] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5322, device='cuda:0') 0.8658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XblbN9jqB72K"
      },
      "source": [
        "import pickle as pk\n",
        "\n",
        "with open('./baseline_train_params.pkl', 'wb') as f:\n",
        "  pk.dump({\n",
        "      'history': history,\n",
        "      'num_of_epoch': 128,\n",
        "      'lr': 0.001,\n",
        "      'momentum': 0.9,\n",
        "      'weight_decay':5e-4\n",
        "  }, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h2IB8n7FOoH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}